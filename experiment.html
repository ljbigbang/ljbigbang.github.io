<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Experiment</title>
    <style>
        *{
            padding: 0;
            margin: 0;
            box-sizing: border-box;
            text-decoration: none;
        }
        body{
            position: relative;
        }
        .title_navig_wrap{
            position: fixed;
            width: 100%;
            height: calc(100vw * 80/1440);
            z-index: 5;
            display: block;
        }
        .title_navig_wrap:hover .title_navig{
            display: block;
        }
        .title_navig{
            position: fixed;
            width: 100%;
            height: calc(100vw * 80/1440);
            background-color: rgba(0, 0, 0, 0.6);
            z-index: 4;
            display: none;
        }
        .title_navig_logo{
            position: absolute;
            height: calc(100vw * 40/1440);
            width: calc(100vw * 40/1440);
            top: calc(100vw * 20/1440);
            left: calc(100vw * 20/1440);
            z-index: 5;
        }
        .title_navig_logo1{
            position: absolute;
            height: calc(100vw * 40/1440);
            width: calc(100vw * 90/1440);
            top: calc(100vw * 23/1440);
            left: calc(100vw * 75/1440);
            z-index: 5;
        }
        .title_navig_logo2{
            position: absolute;
            height: calc(100vw * 80/1440);
            width: calc(100vw * 80/1440);
            padding: calc(100vw * 20/1440);
            top: 0;
            right: 0;
            z-index: 5;
            display: block;
            /* background-image: url("./images/icon_mulu.png"); */
            /* background-repeat: no-repeat; */
            /* background-position: center center; */
            /* background-size: cover; */
        }
        .title_navig_logo2 img{
            position: absolute;
            height: calc(100vw * 40/1440);
            width: calc(100vw * 40/1440);
        }
        .directory{
            position: absolute;
            height: calc(100vw * 120/1440);
            width: calc(100vw * 150/1440);
            top: calc(100vw * 80/1440);
            right: 0;
            visibility: hidden;
            /* display: none; */
            z-index: 5;
            font-size: calc(100vw * 20/1440);
            font-weight: bolder;
            color: white;
            text-align: center;
            list-style-type: none;
        }
        .title_navig_logo2:hover .directory{
            visibility: visible;
        }
        /* .title_navig_logo2:hover .my_video{
            visibility: visible;
        }
        .title_navig_logo2:hover .my_blogs{
            visibility: visible;
        } */
        .my_video{
            position: absolute;
            height: calc(100vw * 60/1440);
            width: calc(100vw * 150/1440);
            top: 0;
            line-height: calc(100vw * 60/1440);
            background-color: rgba(0, 0, 0, 0.6);
            /* display: none; */
            /* visibility: hidden; */
        }
        .my_video:hover{
            background-color: rgba(255, 255, 255, 0.6);
            color: black;
        }
        .my_blogs{
            position: absolute;
            height: calc(100vw * 60/1440);
            width: calc(100vw * 150/1440);
            top: calc(100vw * 60/1440);
            line-height: calc(100vw * 60/1440);
            background-color: rgba(0, 0, 0, 0.6);
            /* display: none; */
            /* visibility: hidden; */
        }
        .my_blogs:hover{
            background-color: rgba(255, 255, 255, 0.6);
            color: black;
        }
        .navigation{
            position: fixed;
            height: 100%;
            /* width: calc(100vw * 440/1440); */
            width: 30%;
            /* border-radius: calc(100vw * 25/1440); */
            background-color: rgba(0, 0, 0, 0.8);
            left: 0;
            display: inline-block;
            color: white;
            /* top: 0; */
            /* background-image: url("./images/Introduction_background2.jpg"); */
            /* background-repeat: no-repeat; */
            /* background-position: center center; */
            /* background-size: cover; */
            /* z-index: 3; */
            /* background-attachment: fixed; */
        }
        .navigation_title{
            position: relative;
            height: calc(100vw * 180/1440);
            width: calc(100vw * 300/1440);
            display: flex;
            /* border: calc(100vw * 5/1440) solid white; */
            margin-top: calc(100vw * 65/1440);
            margin-left: calc(100vw * 40/1440);
            /* padding-bottom: calc(100vw * 50/1440); */
            /* background-color: rgb(30, 30, 30); */
            font-size: calc(100vw * 65/1440);
            font-weight: bolder;
            /* text-align: center; */
            line-height: calc(100vw * 200/1440);
            /* border-radius: calc(100vw * 25/1440); */
        }
        li{
            position: relative;
            height: calc(100vw * 30/1440);
            width: calc(100vw * 365/1440);
            display: flex;
            margin-left: calc(100vw * 40/1440);
            margin-top: calc(100vw * 5/1440);
            margin-bottom: calc(100vw * 10/1440);
            font-size: calc(100vw * 17/1440);
            font-weight: bolder;
            line-height: calc(100vw * 20/1440);
        }
        li:hover{
            color: rgb(223, 179, 17);
        }
        span{
            position: relative;
            /* height: 100%; */
            width: calc(100vw * 1000/1440);
            float: right;
            display: none;
            /* display: inline-block; */
            /* top: calc(100vw * 150/1440); */
            /* margin-left:  calc(100vw * 80/1440); */
            /* margin-right:  calc(100vw * 80/1440); */
            /* margin-bottom: calc(100vw * 80/1440); */
            /* background-color: rgba(255, 255, 255, 0.55); */
            /* backdrop-filter: blur(20px); */
            /* background-image: url("images/Introduction_background3.jpg"); */
            /* background-repeat: no-repeat; */
            /* background-position: center center; */
            /* background-size: cover; */
        }
        .main{
            display: block;
        }
        .content img{
            position: relative;
            width: calc(100vw * 800/1440);
            margin-top: calc(100vw * 10/1440);
            margin-bottom: calc(100vw * 10/1440);
            /* height: calc(100vw * 550/1440); */
            /* margin-left: calc(100vw * 100/1440); */
            /* margin-right: calc(100vw * 100/1440); */
            /* margin-top: calc(100vw * 30/1440); */
            /* margin-bottom: calc(100vw * 80/1440); */
        }
        .content{
            position: relative;
            /* width: calc(100vw * 800/1440); */
            /* height: calc(100vw * 550/1440); */
            margin-left: calc(100vw * 100/1440);
            margin-right: calc(100vw * 100/1440);
            margin-top: calc(100vw * 50/1440);
            margin-bottom: calc(100vw * 50/1440);
            /* padding: calc(100vw * 50/1440); */
            /* border-radius: calc(100vw * 25/1440); */
            /* background-color: rgb(245, 245, 245); */
            /* box-shadow: 10px 10px 32px 0 rgb(0, 0, 0, 0.75); */
            /* backdrop-filter: blur(20px); */
        }
        .content_title{
            position: relative;
            margin-left: calc(100vw * 100/1440);
            margin-right: calc(100vw * 100/1440);
            margin-top: calc(100vw * 50/1440);
            font-size: calc(100vw * 45/1440); 
            border: calc(100vw * 3/1440) solid rgba(0, 0, 0, 0.8);
            border-radius: calc(100vw * 15/1440); 
            display: inline-block; 
            padding: calc(100vw * 10/1440);
        }
        .content_title:hover{
            background-color: rgba(0, 0, 0, 0.8);
            color: white;
        }
        .content p{
            /* text-indent: 2em; */
            font-size: calc(100vw * 20/1440);
            font-weight: bold;
            color: rgb(87, 87, 87);
        }
        .click_style{
            color: rgb(223, 179, 17);
        }
        ul{
            display: none;
        }
        .click_style1{
            display: block;
        }
    </style>
        
    <script>
        window.onload = function(){
        var my_li = document.getElementsByTagName("li");
        var my_span = document.getElementsByTagName("span");
        // var my_ul = [];
        // for(k=0; k<4; k++){
        //     var num = k+1;
        //     // console.log(num);
        //     var str = String(num);
        //     // console.log(str);
        //     my_ul[k]=document.getElementById(str);
        //     my_ul[k].className = "";
        // }
        var i, j;
        for(i=0; i<my_li.length; i++){
            my_li[i].index = i
            my_li[i].onclick = function(){
                for(j=0; j<my_li.length; j++){
                    my_li[j].className="";
                    my_span[j].className="";
                    // my_ul[j].className="";
                }
                this.className += "click_style";
                my_span[this.index].className += "main";
                // my_ul[this.index].className += "click_style1"
                }
            }
        }
    </script>
</head>
<body>
    <div class="title_navig_wrap">
    <div class="title_navig">
        <img class="title_navig_logo" src="./images/QH_LOGO.png" alt="">
        <img class="title_navig_logo1" src="./images/QH_LOGO2.png" alt="">
        <div class="title_navig_logo2">
            <img src="./images/icon_mulu.png" alt="">
            <div class="directory">
                <div class="my_video" onClick="window.location.href = './index.html'; window.event.returnValue = false;">OUR VIDEO</div>
                <div class="my_blogs">OUR BLOGS</div>
            </div>
        </div>
    </div>
    </div>
    <div class="navigation">
        <div class="navigation_title">Experiment</div>
        <ul style="display: block;">
            <li class="click_style">Experimental Setup</li>
            <!-- <ul id="1">
                <li>BaseLine</li>
                <li>Evaluation</li>
            </ul> -->
            <li>Characterization Evaluation</li>
            <li>Quality Evaluation</li>
            <li>Conclusion</li>
        </ul>
    </div>
        <span class="main">
            <p class="content_title">EXPERIMENTAL SETUP</p>
            <div class="content">
                <p style="font-size: calc(100vw * 30/1440);">BaseLine</p>
                <p style="margin-top: calc(100vw * 20/1440);">We use the multimodal data collected in our dataset to train the baseline DeepCharacter, and separate models are trained for different characters, respectively. To pretrain multimodal feature representation models, we utilize multimodal features and emotion labels from CMU-MOSEI dataset. We set the batch size to 8 and hidden feature size to 64, and use the original default settings. The encoded 64-dimension audio/vision features are concatenated with 128-dimension word embeddings transformed by pretrained GPT2 tokenizer. Then we use this multimodal input to finetune DialoGPT-small conversation response generation model.</p>
                <p style="margin-top: calc(100vw * 20/1440);">As for the personalized speech synthesis model, the audios are transformed to 22050Hz sample rate first and aligned with utterances using Montreal ForcedAligner(MFA). Videos should be transformed to 25fps and wavfiles to 16kHz for Pose Controllable Talking Face Generation, and all other settings by default. All models are implemented by PyTorch and performed on a RTX-3090GPU.</p>
            </div>
            <div class="content">
                <p style="font-size: calc(100vw * 30/1440);">Evaluation</p>
                <p style="margin-top: calc(100vw * 20/1440);">Since our proposed task and baseline method heads towards creating personalized characters, the degree of characterization is a key benchmark to evaluate the model performance. We evaluate both the characterization and quality. In spite of objective experiments, we also conduct subjective experiment to evaluate the user experience. The objective evaluation is on all the 5 DeepCharacters: DeepSheldon, DeepPenny, DeepLeonard, Deep-Howard and DeepRaj. For the subjective evaluation, we conduct a survey on 50 people and choose 30 judges according to their knowledge of the characters.</p>
                <p style="margin-top: calc(100vw * 20/1440);">The survey shows that three characters Sheldon, Leonard and Penny, are more impressed and related conclusion should be more reliable, thus we conduct subjective evaluation on the corresponding 3 DeepCharacters: DeepSheldon, DeepPenny and DeepLeonard. Since the textual response lays the foundation of multimodal response, and psychologists think that language can express inner thoughts and feelings, so it’s possible to distinguish a specific character according to his/her utterances. Therefore, we conduct experiments on both textual and multimodel ones.</p>
            </div>
        </span>
        <span>
            <p class="content_title">CHARACTERIZATION EVALUATION</p>
            <div class="content">
                <p style="font-size: calc(100vw * 30/1440);">Characterization of Textual Responses</p>
                <p style="margin-top: calc(100vw * 20/1440);">As a vivid illustration, we present 2 samples of different characters response to same textual inputs. Character’s personality can be somehow reflected in these different responses: Sheldon is a high IQ scientist who uses scientific terms like ’vortex of entropy’ or ’reaction time’ from time to time; Penny, on the other hand, is a woman with rich affection, preferring interjection like ’Yeah’, ’Oh god’ or ’Sweetie’; Leonard, although also being somewhat bookish, is more like a normal and helpful person who questions strange utterances and tend to give favorable replies. And as proved by characterization clustering, character classification and human evaluation, our DeepCharacter models can generate well-characterized textual responses.</p>
                <img src="./images/dc_response.png" alt="">
                <p style="margin-top: calc(100vw * 20/1440);">Following a previous work on character classification, we use SAGE model to derive weights for words uttered by the characters. SAGE enforces a sparse prior on its parameters, and may be sensitive to infrequent terms in the text. To alleviate this issue, given one character, we experimentally group 100 randomly selected responses from this character, forming a virtual document, and reweight the words inside the document utilizing SAGE. Then we use a pretrained basic BERT model to encode the reweighted virtual documents into 128-dimensional embeddings as the final representation of one sample of the given character’s response. We perform t-SNE on these sampled representations from all the 5 characters and 5 DeepCharacters. Note that during the above process, BERT are fixed.</p>
                <p style="margin-top: calc(100vw * 20/1440);">As shown in result, the responses from 5 characters can be well separately clustered, showing the character’s diversity in terms of speaking style and word preference. Intriguingly, the clusters of responses DeepCharacters aligns quite well with the ones of the corresponding characters, respectively. This verifies that the created DeepCharacters can generate highly personalized textual responses, well capturing the speaking style and word preference.</p>
                <img src="./images/t-SNE.png" alt="">
                <p style="margin-top: calc(100vw * 20/1440);">To provide quantative results, we also build a character classifier based on BERT to automatically evaluate the characterization of text responses generated by our DeepCharacters. The character classifier is trained on 18,000 documents from original corpus. This character classifier achieves 98.3% validation accuracy on origin corpus(900 documents) while getting 86.4% testing accuracy on generated responses (2,000 documents), showing that our DeepCharacters can well capture the characteristics.</p>
                <p style="margin-top: calc(100vw * 20/1440);">For human evaluation, we randomly select 50 response samples from DeepSheldon, DeepPenny and DeepLeonard respectively, and each is paired with general response generated by DialoGPT as a non-personalized baseline, and response generated by other character model as an untargeted character comparison. Each question is presented to at least 3 judges.</p>
                <p style="margin-top: calc(100vw * 20/1440);">The judges are asked to rank each response pair for how well the response matches the given material in speaking style, decision-making style and emotion tendency, using a 3-point Likert-like scale following. As shown in result, all three targeted DeepCharacters gain more preferences over non-personalized DialoGPT and untargeted DeepCharacters, which shows our DeepCharacter’s ability of capturing a specific character’s personality and speaking style.</p>
                <img src="./images/RoHE_textual_response.png" alt="">
            </div>
            <div class="content">
                <p style="font-size: calc(100vw * 30/1440);">Characterization of Multimodal Responses</p>
                <p style="margin-top: calc(100vw * 20/1440);">As characterization in multi-modality is comparatively subjective, we rely on user study to evaluate our DeepCharacter’s capacity in generalizing personalized multimodal responses.</p>
                <p style="margin-top: calc(100vw * 20/1440);">We use DialoGPT finetuned on whole DPCD corpus instead of any specific character, and unfinetuned TTS model as the neutral baseline, while the inference of acoustic-visual synchronization model remains the same as DeepCharacter. The neutral baseline has learned the TV show corpus’s special text distribution and can generate high quality speeches.</p>
                <video style="margin-top: calc(100vw * 25/1440); margin-bottom: calc(100vw * 20/1440);" src="./images/Sheldon.mp4" controls loop></video>
                <p style="margin-top: calc(100vw * 20/1440);">We randomly select 10 video clips generated by DeepSheldon, DeepLeonard and DeepPenny, respectively.Like PC-AVS, the total 30 video clips, paired with baseline results, are handed to all 30 judges. The judges are asked to rank each multimodal response pair base on how well the response matches the target character in terms of speaking style, tone and identity, considering all three modalities comprehensively. As shown in result, all three targeted DeepCharacters gain more preferences over the neutral baseline, which shows our DeepCharacter’s ability of generating characterized multimodal responses.</p>
                <img src="./images/RoHE_multimodal_response.png" alt="">
                <p style="margin-top: calc(100vw * 20/1440);">The above results demonstrate the collected DPCD can support our simple baseline to generate deep personalized characters.</p>
            </div>
        </span>
        <span>
            <p class="content_title">QUALITY EVALUATION</p>
            <div class="content">
                <p style="margin-top: calc(100vw * 20/1440);">We perform automatic and human evaluations on the quality of responses generated by our DeepCharacter model. Here, high quality responses refer to reasonable texts, realistic speeches and videos, and of high naturalness and consistency across all three modalities.</p>
            </div>
            <div class="content">
                <p style="font-size: calc(100vw * 30/1440);">Quality of Textual Responses</p>
                <p style="margin-top: calc(100vw * 20/1440);">Following DialoGPT, we perform automatic evaluations using several popular standard evaluation metrics, including SacreBLEU and Perplexity. SacreBLEU provides hassle-free computation of BLEU scores, ranging from 0 to 100, and a higher SacreBLEU means a better match between generated results and references. Perplexity measures how likely the model is to generate the input text sequence and can be used to evaluate how well the model has learned the distribution of the training text. We also use Entropy and Dist-n to evaluate lexical diversity.</p>
                <p style="margin-top: calc(100vw * 20/1440);">We calculate automatic evaluation scores for DeepSheldon, DeepPenny, DeepLeonard, DeepHoward and DeepRaj separately. In comparison, we evaluate DialoGPT’s results with the same contexts, as a non-personalized general response. We also finetune DialoGPT on the mixed corpus of these characters, to bridge the performance gap caused by corpus difference and serve as an untargeted response. We additionally evaluate the lexical diversity and perplexity of the characters’ original corpus, to learn text distribution.</p>
                <p style="margin-top: calc(100vw * 20/1440);">Table summarizes the automatic evaluation results, showing DeepCharacter’s stronger adaptability to complicated daily problems. All DeepCharacters achieve a higher score in lexical diversity than original DialoGPT, and are close to respective original corpus. Both generated responses and original corpus have higher perplexity than DialoGPT’s results, illustrating the complexity and distribution diversity of our DPCD, which can also be verified by the results of finetuned DialoGPT. This also demonstrates DeepCharacter’s ability to fit text distribution of the original corpus. Due to high complexity and ambiguity of generation task, all five DeepCharacters receive rather low SacreBLEU score, but still higher than original and finetuned DialoGPT, showing a stronger reasoning ability to understand and reply diverse and colloquial dialogues.</p>
                <img src="./images/Automatic_evaluation_scores.png" alt="">
                <p style="margin-top: calc(100vw * 20/1440);">We also conduct human study to evaluate our model’s ability in generating high quality characterized textual responses. The testing data remain same with characterization evaluation. We first evaluate the generated text quality from the views of fluency, context relevant and personalized/emotional degree. Judges rate between 1 to 3 for each aspect, where 1 means ”very bad, not at all fluent/not at all relevant/very general”, 2 means ”not so bad, some minor mistakes”, and 3 means ”very fluent/relevant/personalized or emotional”.</p>
                <p style="margin-top: calc(100vw * 20/1440);">As shown in result, all DeepCharacters reach high scores on all three evaluation criteria. According to significance test, the differences between DeepCharacter and DialoGPT in these three criteria are less significant, indicating that our DeepCharacter can generate responses that are as fluent, reasonable, and emotional as DialoGPT. While DialoGPT is well-known for generating high quality responses given basic and general contexts, our DeepCharacter can achieve similar quality in complicated personalized scenarios.</p>
                <img src="./images/RoHE_text_equlity.png" alt="">
                <p style="margin-top: calc(100vw * 20/1440);">The above results demonstrate that our DeepCharacters achieve comparatively high score in lexical diversity, reasonalibity, fluency and emotionality of generated texts.</p>
            </div>
            <div class="content">
                <p style="font-size: calc(100vw * 30/1440);">Quality of Generated Multimodal Responses</p>
                <p style="margin-top: calc(100vw * 20/1440);">We conduct user study to evaluate the multimodal quality of the generated videos. The data and experiment settings remain the same with the video characterization user study, and the judges are required to rank each multimodal response pair for the responses’ performance in language fluency, naturalness and realness.</p>
                <p style="margin-top: calc(100vw * 20/1440);">As shown in result, all three DeepCharacters gain more preferences over comparative baseline, which confirms ourDeepCharacter’s ability of generating high quality multimodal responses. Although videos generated by both models shares similar speech clarity and image sharpness, the vivid tone and reasonable textual responses generated by our DeepCharacter increase naturalness and realness, resulting in judges’ preference when considering multimodal quality.</p>
                <p style="margin-top: calc(100vw * 20/1440);">To study how the multimodal input affects the response,we take the same textual stimuli mixed with different video/audio context. As shown in result, instead of generating the same response to the shared text, our model gives different results not only in response to the textual context, but also related to the emotion expressed in the video/audio context. Specifically, our DeepCharacter tends to generate a more tender or more positive response when given video/audio inputs expressing sad moods. On the contrary, when the speaker in the video/audio input is very angry and unfriendly, the model will be more likely to generate unkind words, sometimes seeming to fight back, like ”How am I supposed to know that?”, or ”No, I didn’t get it, I thought it was funny. Apparently neither did you.” Extra modalities can offer more effective information, such as the speaker’s facial expressions, voice or tone, which can better express the speaker’s emotion and will certainly affect natural response generation. Given multimodal inputs, the model can obtain more comprehensive information from the conversation context, thus understanding the character’s interaction pattern and personality better, generating diverse and more personalized responses.</p>
                <img src="./images/EoDR_multimodal_inputs.png" alt="">
            </div>
        </span>
        <span>
            <p class="content_title">CONCLUSION</p>
            <div class="content">
                <p style="margin-top: calc(100vw * 20/1440);">Towards a fantastic goal of creating personalized characters with whom one can interact in a multimodal channel, we explore creating personalized characters from collected multimodal data, in a data-driven way. In this project, we formulate a novel task (DPCC), given multimodal-in stimuli, and predict multimodal-out response. To support this task, we collect a multimodal conversation dataset (DPCD) surrounding several characters from TV shows. This dataset provides about 10 times more multimodal conversations of high quality, well aligned across multi-modality. We further provide a baseline solution for learning multimodal response given multimodal input. The created DeepCharacters is capable of generating relatively high-quality response, and consistent personality to the original character. In the future, we would like to collect richer characters’data such as pose, gestures etc, and more rounds of conversations to further enrich the personalized feature. We suppose this work can shed light on an interesting research direction towards creating deep personalized characters to support multimodal chatting scenarios, and inspire further works on, such as, improving the visual and acoustic quality, proposing metrics for evaluating personality similarity and fidelity, extending the 2D video to 3D for more immersive interaction.</p>
            </div>
        </span>
</body>
</html>