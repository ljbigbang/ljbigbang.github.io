<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction</title>
    <style>
        *{
            padding: 0;
            margin: 0;
            box-sizing: border-box;
            text-decoration: none;
        }
        body{
            position: relative;
            background-image: url("./images/Introduction_background.jpg");
            background-repeat: no-repeat;
            background-position: center center;
            background-attachment: fixed;
            background-size: cover;
            /* display: flex; */
            /* padding-bottom: calc(100vw * 100/1440); */
            /* background-color: rgba(15, 15, 15, 0.9); */
            /* background-color: white; */
        }
        .title_navig_wrap{
            position: fixed;
            width: 100%;
            height: calc(100vw * 80/1440);
            z-index: 5;
            display: block;
        }
        .title_navig_wrap:hover .title_navig{
            display: block;
        }
        .title_navig{
            position: fixed;
            width: 100%;
            height: calc(100vw * 80/1440);
            background-color: rgba(0, 0, 0, 0.6);
            z-index: 4;
            display: none;
        }
        .title_navig_logo{
            position: absolute;
            height: calc(100vw * 40/1440);
            width: calc(100vw * 40/1440);
            top: calc(100vw * 20/1440);
            left: calc(100vw * 20/1440);
            z-index: 5;
        }
        .title_navig_logo1{
            position: absolute;
            height: calc(100vw * 40/1440);
            width: calc(100vw * 90/1440);
            top: calc(100vw * 23/1440);
            left: calc(100vw * 75/1440);
            z-index: 5;
        }
        .title_navig_logo2{
            position: absolute;
            height: calc(100vw * 80/1440);
            width: calc(100vw * 80/1440);
            padding: calc(100vw * 20/1440);
            top: 0;
            right: 0;
            z-index: 5;
            display: block;
            /* background-image: url("./images/icon_mulu.png"); */
            /* background-repeat: no-repeat; */
            /* background-position: center center; */
            /* background-size: cover; */
        }
        .title_navig_logo2 img{
            position: absolute;
            height: calc(100vw * 40/1440);
            width: calc(100vw * 40/1440);
        }
        .directory{
            position: absolute;
            height: calc(100vw * 120/1440);
            width: calc(100vw * 150/1440);
            top: calc(100vw * 80/1440);
            right: 0;
            visibility: hidden;
            /* display: none; */
            z-index: 5;
            font-size: calc(100vw * 20/1440);
            font-weight: bolder;
            color: white;
            text-align: center;
            list-style-type: none;
        }
        .title_navig_logo2:hover .directory{
            visibility: visible;
        }
        /* .title_navig_logo2:hover .my_video{
            visibility: visible;
        }
        .title_navig_logo2:hover .my_blogs{
            visibility: visible;
        } */
        .my_video{
            position: absolute;
            height: calc(100vw * 60/1440);
            width: calc(100vw * 150/1440);
            top: 0;
            line-height: calc(100vw * 60/1440);
            background-color: rgba(0, 0, 0, 0.6);
            /* display: none; */
            /* visibility: hidden; */
        }
        .my_video:hover{
            background-color: rgba(255, 255, 255, 0.6);
            color: black;
        }
        .my_blogs{
            position: absolute;
            height: calc(100vw * 60/1440);
            width: calc(100vw * 150/1440);
            top: calc(100vw * 60/1440);
            line-height: calc(100vw * 60/1440);
            background-color: rgba(0, 0, 0, 0.6);
            /* display: none; */
            /* visibility: hidden; */
        }
        .my_blogs:hover{
            background-color: rgba(255, 255, 255, 0.6);
            color: black;
        }
        .head{
            position: relative;
            /* width: 100%; */
            height: calc(100vw * 150/1440);
            /* left: 0; */
            /* top: 0; */
            /* background-image: url("./images/Introduction_background2.jpg"); */
            /* background-repeat: no-repeat; */
            /* background-position: center center; */
            /* background-size: cover; */
            /* z-index: 3; */
            /* background-attachment: fixed; */
        }
        .section_title{
            position: absolute;
            top: calc(100vw * 35/1440);
            left: calc(100vw * 520/1440);
            height: calc(100vw * 80/1440);
            width: calc(100vw * 400/1440);
            text-align: center;
            line-height: calc(100vw * 70/1440);
            color: white;
            border: 5px solid white;
            font-size: calc(100vw * 35/1440);
            font-weight: bolder;
        }
        .section_title:hover{
            background-color: white;
            color: black;
        }
        .wrap{
            position: relative;
            /* height: calc(100vw * 600/1440); */
            width: calc(100vw * 1280/1440);
            /* top: calc(100vw * 150/1440); */
            margin-left:  calc(100vw * 80/1440);
            margin-right:  calc(100vw * 80/1440);
            margin-bottom: calc(100vw * 80/1440);
            background-color: rgba(255, 255, 255, 0.55);
            /* backdrop-filter: blur(20px); */
            /* background-image: url("images/Introduction_background3.jpg"); */
            /* background-repeat: no-repeat; */
            /* background-position: center center; */
            /* background-size: cover; */
        }
        .navigation{
            position: relative;
            height: calc(100vw * 60/1440);
            width: 100%;
            top: 0;
            left: 0;
            display: -webkit-flex;
            display: flex;
            /* background-color: rgba(23, 12, 12, 0.6); */
            /* z-index: 1; */
        }
        span{
            position: relative;
            /* height: calc(100vw * 600/1440); */
            width: 100%;
            padding-bottom: calc(100vw * 10/1440);
            display: none;
            /* top: calc(100vw * 60/1440); */
            /* display: block; */
            /* background-color: blue; */
            /* z-index: 1; */
        }
        .main{
            display: block;
        }
        .theme_title{
            position: relative;
            margin-top: calc(100vw * 80/1440);
            margin-bottom: calc(100vw * 80/1440);
            text-align: center;
            font-size: calc(100vw * 80/1440);
            font-weight: bolder;
            font-family: 'Arial Narrow', Arial, sans-serif;
        }
        .content{
            position: relative;
            width: calc(100vw * 980/1440);
            /* height: calc(100vw * 550/1440); */
            margin-top: calc(100vw * 80/1440);
            margin-left:  calc(100vw * 150/1440);
            margin-bottom:  calc(100vw * 100/1440);
            margin-right:  calc(100vw * 150/1440);
            padding: calc(100vw * 70/1440);
            border-radius: calc(100vw * 25/1440);
            /* margin: calc(100vw * 100/1440),calc(100vw * 220/1440),calc(100vw * 100/1440),calc(100vw * 220/1440); */
            background-color: rgb(245, 245, 245);
            box-shadow: 10px 10px 32px 0 rgb(0, 0, 0, 0.75);
            backdrop-filter: blur(20px);
        }
        .content p{
            text-indent: 2em;
            font-size: calc(100vw * 30/1440);
            font-weight: bold;
            color: rgb(33, 33, 33);
        }
        ul{
            /* position: relative; */
            /* height: calc(100vw * 60/1440); */
            display: flex;
            font-size: 0;
        }
        li{
            position: relative;
            /* width: calc(100vw * 100/1440); */
            /* height: 100%; */
            height: calc(100vw * 60/1440);
            text-align: center;
            line-height: calc(100vw * 60/1440);
            font-size: calc(100vw * 20/1440);
            font-weight: bold;
            color: white; 
            background-color: rgba(60, 43, 43, 0.6);
            /* display: inline-block; */
            float: left;
            padding-left: calc(100vw * 20/1440);
            padding-right: calc(100vw * 20/1440);
            list-style-type: none;
        }
        .navigation_holder{
            position: relative;
            /* list-style-type: none; */
            /* height: 100%; */
            height: calc(100vw * 60/1440);
            width: 100%;
            /* width: calc(100vw * 680/1440); */
            display: flex;
            /* align-content: stretch; */
            /* float: right; */
            /* width: calc(100vw * 250/1440); */
            background-color: rgba(60, 43, 43, 0.6);
        }
        .icon1{
            position: absolute;
            height: calc(100vw * 30/1440);
            width: calc(100vw * 30/1440);
            margin: calc(100vw * 15/1440);
            right: 0;
        }
        .icon2{
            position: absolute;
            height: calc(100vw * 30/1440);
            width: calc(100vw * 30/1440);
            margin: calc(100vw * 15/1440);
            right: calc(100vw * 50/1440);
        }
        .icon3{
            position: absolute;
            height: calc(100vw * 30/1440);
            width: calc(100vw * 30/1440);
            margin: calc(100vw * 15/1440);
            right: calc(100vw * 100/1440);
        }
        .click_style{
            background-color: rgba(0, 0, 0, 0);
            /* background-color: rgba(0, 0, 0, 0.75); */
            color: rgb(33, 33, 33);
        }
    </style>
        
    <script>
        window.onload = function(){
        var my_li = document.getElementsByTagName("li");
        var my_span = document.getElementsByTagName("span");
        var i, j;
        for(i=0; i<my_li.length; i++){
            my_li[i].index = i
            my_li[i].onclick = function(){
                for(j=0; j<my_li.length; j++){
                    my_li[j].className="";
                    my_span[j].className="";
                }
                this.className += "click_style";
                my_span[this.index].className += "main";
            }
        }
    }
    </script>
</head>
<body>
    <div class="title_navig_wrap">
    <div class="title_navig">
        <img class="title_navig_logo" src="./images/QH_LOGO.png" alt="">
        <img class="title_navig_logo1" src="./images/QH_LOGO2.png" alt="">
        <div class="title_navig_logo2">
            <img src="./images/icon_mulu.png" alt="">
            <div class="directory">
                <div class="my_video" onClick="window.location.href = './index.html'; window.event.returnValue = false;">OUR VIDEO</div>
                <div class="my_blogs">OUR BLOGS</div>
            </div>
        </div>
    </div>
    </div>
    <div class="head">
        <div class="section_title">Introduction</div>
    </div>
    <div class="wrap">
        <div class="navigation">
            <ul>
                <li class="click_style">Abstract</li>
                <li>Our&nbsp;Goals</li>
                <li>About&nbsp;DPCC</li>
                <li>About&nbsp;DPCD</li>
                <li>BaseLine&nbsp;&&nbsp;Evaluation</li>
                <li>Related&nbsp;Work</li>
            </ul>
            <div class="navigation_holder">
                <img class="icon1" src="./images/guanbi.png" alt="">
                <img class="icon2" src="./images/maximize.png" alt="">
                <img class="icon3" src="./images/minimum.png" alt="">
            </div>
        </div>
        <span class="main">
            <div class="theme_title">ABSTRACT</div>
            <div class="content">
                <p> Imagine an interesting multimodal interactive scenario that you can see, hear, and chat with an AI-generated digital character, who is capable of behaving like Sheldon from The Big Bang Theory, as a DEEP copy from appearance to personality. Towards this fantastic multimodal chatting scenario, we propose a novel task, named Deep Personalized Character Creation (DPCC): creating multimodal chat personalized characters from multimodal data such as TV shows. Specifically, given a single- or multi-modality input (text, audio, video), the goal of DPCC is to generate a multi-modality (text, audio, video) response, which should be well-matched the personality of a specific character such as Sheldon, and of high quality as well.</p>
            </div>
            <div class="content">
                <p>To support this novel task, we further collect a character centric multimodal dialogue dataset, named Deep Personalized Character Dataset (DPCD), from TV shows. DPCD contains character-specific multimodal dialogue data of 10k utterances and 6 hours of audio/video per character, which is around 10 times larger compared to existing related datasets. On DPCD, we present a baseline method for the DPCC task and create 5 Deep personalized digital Characters (DeepCharacters) from Big Bang TV Shows. We conduct both subjective and objective experiments to evaluate the multimodal response from DeepCharacters in terms of characterization and quality. The results demonstrates that, on our collected DPCD dataset, the proposed baseline can create personalized digital characters for generating multimodal response. </p>
            </div>
        </span>
        <span>
            <div class="theme_title">OUR&nbsp;GOALS</div>
            <div class="content">
                <p> We expect to interact with an AI-generated personalized character, just feels like interacting with a specific person, and the AI-generated character can respond to you with personalized text, voice and facial expression. Instead of creating a totally factitious character from manual settings, in this project, we are heading for a novel setting: creating a DEEP personalized specific character from his/her dialog videos providing textual, vocal and visual data. By DEEP, we mean not only mimicing the outer characteristics such as appearance and tune, but also the inner characteristics such as personality. Once we get a DEEP personalized Sheldon (DeepSheldon), one can have a multimodal chat with him and feel like we are really talking to the one you know from The Big Bang Theory. </p>
            </div>
        </span>
        <span>
            <div class="theme_title">ABOUT&nbsp;DPCC</div>
            <div class="content">
                <img style="position: relative; width: 100%; height: 100%;" src="./images/DPCC_process.png" alt="">
                <p style="margin-top: calc(100vw * 30/1440);">Towards our fantastic goal, we propose a novel task, named Deep Personalized Character Creation (DPCC), as generating a personalized character from the multimodal conversation data including text, audio, and video. DPCC takes the multimodal stimuli as input, and responds with a personalized multimodal output. Given the same input, different Deep personalized digital Characters (DeepCharacters) should provide responses that well reflect their individual trait, respectively. With multimodal response, DeepCharacters can bring more concrete and immersive interactive experience, compared to previous chatbots, which can only provide text or voice response. DPCC can also support text-only input and generate multimodal response. Also, different from previous personalized conversation generation tasks, our DPCC doesn’t require additional profiles or traits as input to explicitly label personality, and learn the character’s interaction pattern, speaking style and personality from conversation contexts.</p>
            </div>
        </span>
        <span>
            <div class="theme_title">ABOUT&nbsp;DPCD</div>
            <div class="content">
                <p>To support DPCC task, we collect a multimodal conversation dataset, named Deep Personalized Character Dataset(DPCD). DPCD containing 5 main characters from The Big Bang Theory, and in total 29.75 hours videos and 29k conversation turns. We only collect the video clips containing characters’ talking face to extract effective facial expression features. Besides, we manually check the auto-alignment between text and audio/video modality. We also conduct preprocessing to remove noise in audio. Those efforts ensure the collected data is of high quality for DPCC task. There do exist multimodal (text, audio, video) datasets for natural language processing(NLP), audio-visual synchronization (AVS) and speech synthesis. These existing multimodal datasets are collected for a general understanding of conversation content, emotion, or general synthesis of speech and talking head that can be applied to different persons, not for modelling specific characters. Consequently, the data for modeling the personality of a specific character is typically insufficient. For some datasets, the character labels of conversations are even not provided. But our DPCD focus on the main characters and can provide almost an order more (9,718 utterances per character). </p>
            </div>
        </span>
        <span>
            <div class="theme_title">BASELINE&nbsp;&&nbsp;EVALUATION</div>
            <div class="content">
                <p>We present a baseline solution for DPCC task and create 5 DeepCharacters from the collected DPCD. To evaluate the textual and multimodal response from DeepCharacters in terms of characterization and quality, we design some tools to evaluate the characterization quality. Both subjective and objective experimental results demonstrate that the response of our created DeepCharacters can well reflect the personality of characters, indicating DPCD can be used for DPCC task.</p>
            </div>
        </span>
        <span>
            <div class="theme_title">RELATED&nbsp;WORK</div>
            <div class="content">
                <p>Modeling personalized conversation has long been a complicated issue. 
                Li et al. address the challenge of persona consistency by encoding personas and adding persona embeddings into seq2seq model. They generate persona-specific responses based on the conversations of 13 characters from TV series Friends and The Big Bang Theory. 
                Based on the persona-based model by Li et al., Xing et al. estimate the OCEAN scores for each speaker to train persona embedding.</p>
            </div>
            <div class="content">
                <p>Considering the difficulty and complexity of persona-based conversation modeling, many models rely on extra profile or personality traits to develop a sense of persona. 
                Mazar ́e et al. define persona as a set of sentences representing the personality of the responding agent and train persona-based dialogue models on a large-scale persona-based dialogue dataset (PCR) extracted from RED-DIT2. 
                Zhong et al. further extend PCR with annotated empathy information. 
                Zheng et al. build a persona-based empathetic conversation response selection model from amounts of multi-turn empathetic conversation data with five personality annotations (i.e.Gender, Age, Location, Interest Tags, and Self-description) for each speaker.
                Wu et al. propose a generative split memory network to incorporate diverse personal information and generate personalized responses based on a personalized dataset PERCHAT with finer-grained user personal information and contextual comments. 
                Chen et al. build the personality detection and emotion recognition model from a Chinese personalized dataset CPED with annotations of speakers’personalities, dynamic emotions and dialog actions.</p>
            </div>
            <div class="content">
                <img style="position: relative; width: 100%; height: 100%;" src="./images/DataSet_details_comparison.png" alt="">
                <p style="margin-top: calc(100vw * 30/1440);">Constrained by the lack of single speaker’s conversation data and the complexity of personality modeling, the models listed above mainly focus on personal information consistency rather than character consistency, and rely heavily on detailed profiles. Different from them, we directly train a deep personalized character model from a rather larger volume of multimodal conversation data for a single speaker. Detailed comparisons of our multimodal dataset DPCD to the related datasets are presented above.</p>
            </div>
        </span>
    </div>
</body>
</html>